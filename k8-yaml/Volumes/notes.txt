

Kubernetes Volumes ‚Äì Simple Explanation
=======================================

‚ùì Why Volumes Are Needed
-------------------------
- Pods in Kubernetes are temporary (transient).
- When a Pod is deleted, any data stored inside it is also lost.
- If a Pod processes data and saves it, that data will disappear unless we take steps to preserve it.
- To retain data, we use a Volume.

üíæ What Happens When We Use a Volume
------------------------------------
- A Volume is like a separate storage unit attached to a Pod.
- Even if the Pod is deleted, the data in the volume remains.
- Volumes help keep important files like logs, output files, etc.

üß™ Example: Pod Writing a Random Number
---------------------------------------
- We create a Pod that:
  - Generates a random number between 1 and 100
  - Writes that number to a file: /opt/number.out
- Without a Volume:
  - The file is deleted when the Pod is deleted.
- With a Volume:
  - The file is stored outside the Pod, and it persists.

üß∞ Volume Implementation (Single Node Cluster)
----------------------------------------------
1. Use a hostPath volume ‚Äî maps a directory from the host machine.
2. Choose /data on the host as the storage location.
3. Inside the Pod's container, mount this volume to /opt.

‚úÖ Outcome:
- The file /opt/number.out inside the container is saved as /data/number.out on the host.
- Even if the Pod is deleted, the file still exists on the node.

üìÑ YAML Example
---------------
apiVersion: v1
kind: Pod
metadata:
  name: random-number-pod
spec:
  containers:
    - name: random-generator
      image: busybox
      command: ["/bin/sh", "-c"]
      args:
        - RANDOM_NUM=$(expr $RANDOM % 100 + 1);
          echo $RANDOM_NUM > /opt/number.out;
          sleep 3600;
      resources:
        limits:
          memory: "128Mi"
          cpu: "500m"
      volumeMounts:
        - name: data-volume 
          mountPath: /opt
  volumes:
    - name: data-volume 
      hostPath:  # instead of hostPath, you can use emptyDir, configMap, cloud storages, etc.
        path: /data
        type: Directory

‚ö†Ô∏è Limitation of hostPath Volumes
---------------------------------
- This setup only works well in single-node clusters.
- In a multi-node cluster:
  - Each node has its own /data directory.
  - Data won't be shared between nodes.
  - Pods on different nodes won‚Äôt see the same data.

------------------------------------------------------
Kubernetes supports many advanced storage options that work across nodes:

- Network storage:
  - NFS (Network File System)
  - GlusterFS
  - CephFS
  - iSCSI
  - Fibre Channel
- Cloud storage:
  - AWS: EBS (Elastic Block Store)
  - Azure: Disk, File
  - Google Cloud: Persistent Disk

üì¶ Example: AWS EBS Volume
--------------------------
To use an AWS EBS volume:

Replace hostPath with:
awsElasticBlockStore:
  volumeID: <your-volume-id>
  fsType: ext4

This mounts a real AWS EBS volume into your Pod.

‚ùì Problem with Pod-defined Volumes
----------------------------------
- Previously, volumes were defined **inside Pod YAML** files.
- This means each Pod had to define its own storage config.
- In large environments:
  - Users must repeat storage configs in every Pod.
  - Any storage changes require updating all Pod files.
- This is **hard to manage and not scalable**.

-----------------------------------
**** Persistent Volumes (PV) ****          
-----------------------------------
- PVs allow **centralized storage management**.
- Created and managed by **cluster administrators**.
- Users request storage using **Persistent Volume Claims (PVC)**.
- **Decouples** storage config from individual Pods.

üì¶ What is a Persistent Volume (PV)?
------------------------------------
- A PV is a **cluster-wide storage resource** not at namespace
- Pre-configured by an administrator.
- Acts as a **pool of storage** available to users.
- Users request portions using PVCs.

üõ† PV Specification ‚Äì Key Fields
--------------------------------
apiVersion: v1           # API version used for PersistentVolume
kind: PersistentVolume   # Specifies the resource type as PersistentVolume
metadata:
  name: pv-vol-one       # Name of the PersistentVolume

spec:
  capacity:              # Specifies the storage capacity for the volume
    storage: 1Gi         # Total size allocated for this PV

  accessModes:           # Describes how the volume can be mounted
    - ReadWriteOnce      # RWO: can be mounted as read-write by a single node

  hostPath:              # Defines the type of storage; hostPath uses local disk
    path: /tmp/data      # Local path on the node used for storing data
                         # Suitable only for single-node or testing setups


üìå Notes
--------
- `hostPath` is only suitable for **local testing**.
- In production, replace with:
  - `awsElasticBlockStore` (AWS)
  - `gcePersistentDisk` (GCP)
  - `azureDisk` (Azure)
  - NFS, iSCSI, CephFS, etc.

Access Modes define HOW a Pod can access a volume:
- Can it READ or WRITE?
- Can ONE Pod use it, or MANY?
- Can it be mounted on ONE node or MULTIPLE?

Types of Access Modes
----------------------

1. ReadWriteOnce (RWO)
   - One Pod can READ and WRITE the volume
   - But only from ONE node at a time

2. ReadOnlyMany (ROX)
   - MANY Pods can READ the volume
   - But none can WRITE to it

3. ReadWriteMany (RWX)
   - MANY Pods can READ and WRITE to the volume
   - At the same time


üîß Commands
-----------
# Create a PV from YAML
kubectl create -f pv.yaml

# List all persistent volumes
kubectl get pv

------------------------------------------
**** Persistent Volume Claims (PVC) ****
------------------------------------------

- PVC (Persistent Volume Claim) is a **user's request** for storage.
- It is a **namespaced resource**.
- PVs and PVCs are **separate objects** in Kubernetes and are linked (bound) when a match is found.

üîó Binding Process
-------------------
- Kubernetes **automatically binds** PVCs to suitable PVs.
- Match is based on:
  - Requested storage size
  - Access mode (e.g., ReadWriteOnce)
  - Storage class
  - Volume mode

üß† Important Rules
-------------------
- One PVC binds to only **one PV**.
- Even if the PV has more storage than requested, the rest **cannot be shared**.
- If no matching PV is found, PVC stays in a **Pending** state.
- When a new PV becomes available, Kubernetes **automatically binds** it.

üéØ Manual Binding with Labels
------------------------------
- You can use **labels and selectors** to match PVCs to specific PVs.
- Helpful when multiple PVs are available and you want to control the binding.

üìÑ Example: Create a PVC
-------------------------
apiVersion: v1                          # API version for PVC
kind: PersistentVolumeClaim             # Specifies that this is a PVC
metadata:
  name: my-claim                        # Name of the PVC
spec:
  accessModes:
    - ReadWriteOnce                     # Only one node can mount the volume read-write
  resources:
    requests:
      storage: 500Mi                    # Requesting 500 MB of storage

üì¶ PVC Binds to PV
-------------------
- If there is a PV with 1Gi capacity and RWO mode:
  - PVC asking for 500Mi can still be **bound** to it.
  - Remaining 500Mi in PV will not be shared or reused by other claims.

üîß Commands
-----------
# Create PVC:
kubectl create -f pvc.yaml

# View PVCs:
kubectl get pvc

# Sample PVC output:
NAME        STATUS    VOLUME       CAPACITY   ACCESS MODES   STORAGECLASS   AGE
my-claim    Bound     pv-vol-one   1Gi        RWO            standard        10s

# View PVs:
kubectl get pv

# Sample PV output:
NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM             STORAGECLASS   AGE
pv-vol-one   1Gi        RWO            Retain           Bound    default/my-claim  standard        1m

# Delete a PVC:
kubectl delete pvc my-claim

üßπ What Happens to the PV After PVC is Deleted?
-----------------------------------------------
- The **reclaimPolicy** field in the PV controls what happens after the PVC is deleted.

Reclaim Policy Options (used in YAML under spec.persistentVolumeReclaimPolicy):

1. Retain (default):
   - Value in YAML: Retain
   - PV remains and keeps the data.
   - Must be manually deleted or reused by admin.

2. Delete:
   - Value in YAML: Delete
   - PV and the underlying storage are deleted automatically.
   - Frees up space.

3. Recycle (deprecated):
   - Value in YAML: Recycle
   - PV is scrubbed and reused.
   - Not commonly used in modern setups.

‚úÖ Summary
----------
Concept     - Description
----------------------------------------------
Pod         - Temporary unit of execution, data is lost on delete
Volume      - Storage attached to Pod, used to retain data
hostPath    - Uses host machine folder for storage (not for multi-node use)
Cloud Volume- Works across nodes, ideal for production
Use Case    - Saving generated data like logs, outputs, etc. even after Pod deletion

‚úÖ Persistent Volumes (PV) solve this by decoupling storage from Pods.

------------------------------------------------------------
üìò What are they?
------------------------------------------------------------

1Ô∏è‚É£ PersistentVolume (PV)
--------------------------
- A piece of storage in the cluster provisioned by an admin or dynamically.
- Lives outside of the Pod lifecycle.
- Can be NFS, iSCSI, cloud disk (EBS, GCE, etc.)
- PV is a **cluster-scoped** resource (not namespaced).

2Ô∏è‚É£ PersistentVolumeClaim (PVC)
-------------------------------
- A request for storage by a user (Pod).
- Specifies size, access mode, storage class, etc.
- Binds to a matching PV.
- PVC is **namespace-scoped**.

3Ô∏è‚É£ StorageClass
----------------
- Defines how storage is dynamically provisioned.
- Each class may map to a different backend (fast SSDs, standard HDD, etc.)
- Used in **dynamic provisioning** (PVC automatically creates PV).

------------------------------------------------------------
üîÅ Static vs Dynamic Provisioning
------------------------------------------------------------

üõ† Static Provisioning:
- Admin manually creates PV.
- User creates a PVC that matches it.
- Binding is done if specs match.

‚öôÔ∏è Dynamic Provisioning:
- User creates a PVC with a StorageClass.
- Kubernetes automatically creates and binds a PV.
- No need for the admin to pre-create PVs.


Pod using PVC
-------------------
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: nginx
      volumeMounts:
        - mountPath: /usr/share/nginx/html
          name: my-storage
  volumes:
    - name: my-storage
      persistentVolumeClaim:
        claimName: my-pvc

‚û°Ô∏è Explanation:
- Mounts the PVC my-pvc at `/usr/share/nginx/html` inside the Pod.


StorageClass (for Dynamic Provisioning)
--------------------------------------------
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2

‚û°Ô∏è Explanation:
- `provisioner`: Defines the plugin for dynamic provisioning.
- `parameters`: Backend-specific details.

---

5Ô∏è‚É£ PVC with StorageClass (Dynamic)
-------------------------------------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: dynamic-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: standard
  resources:
    requests:
      storage: 2Gi

‚û°Ô∏è Explanation:
- Requests 2Gi of storage.
- Uses `standard` StorageClass for dynamic provisioning.
- No need to pre-create a PV ‚Äî Kubernetes will do it.

------------------------------------------------------------
üß† Summary
------------------------------------------------------------

| Component       | Purpose                                      | Scope         |
|----------------|----------------------------------------------|---------------|
| PV              | Actual physical volume (cluster resource)    | Cluster-wide  |
| PVC             | Claim/request for storage                    | Namespaced    |
| StorageClass    | How dynamic provisioning should happen       | Cluster-wide  |
| Pod             | Mounts PVC as a volume                       | Namespaced    |
